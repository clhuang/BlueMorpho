\documentclass[10pt,twocolumn]{article} 
\usepackage{simpleConference}
\usepackage{amsmath}
\usepackage{times}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{url,hyperref}

\begin{document}

\title{Extending Morphological Chains with Supervised Learning and Cross-Language Features}

\author{Calvin Huang (calvinh@mit.edu)\\
    Brian Shimanuki (bshimanuki@mit.edu)\\
    Charlotte Chen (czchen@mit.edu)
\\
}

\maketitle
\thispagestyle{empty}

\begin{abstract}
    Building on an existing algorithm to perform morphological analysis,
    based on using semantic features and contrastive estimation to detect morphological chains,
    we investigate the effects of various types of extensions to this algorithm.
    We began by adding certain features to the model and heuristics for prefix/suffix selection.
    In addition, we extend the unsupervised model with the log-likelihood of known word segmentations,
    producing a semi-supervised model.
    Finally, we also attempt to use Turkish morphological parents and an English-Turkish translation
    dictionary to help detect irregular English word segmentations and transformations.
\end{abstract}

% TODO explain existing code
\section{Introduction}
Morphologically related words\dots %%TODO

\section{Background Work}
Our model builds on work done by Narasimhan et al. (2015) in constructing \emph{morphological chains} to model the morphological segmentation of words in a language in an unsupervised manner. The idea behind morphological chains is that complex words are constructed by attaching morphemes simpler words. Thus, complex words have words from which they are directly derived, which we call \emph{parent} words. Likewise, the derived word is a \emph{child} word. A morphological chain is a sequence of words such that consecutive pairs form a parent-child relationship. For example, the word \emph{unsustainable} can be constructed from \emph{sustain}, as demonstrated in the chain \emph{sustain} $\rightarrow$ \emph{sustainable} $\rightarrow$ \emph{unsustainable}. Words without parents are called \emph{base} words. In our example, \emph{sustain} is a base word.

Since a parent word can have multiple children (e.g., \emph{play} $\rightarrow$ \emph{plays} and \emph{play} $\rightarrow$ \emph{played}), words can be part of multiple chains. Thus multiple chains can share segments. Narasimhan makes use of this shared information by constructing a model which analyzes parent-child pairs.

\subsection{Model}

A log-linear model is used to evaluate different pairs. For this, a feature mapping $\phi: \mathcal W \times \mathcal Z \rightarrow \mathbb R^d$ to compute a corresponding weight vector $\theta\in\mathbb R^d$. $\mathcal W$ is the set of words being trained on, and $\mathcal Z$ is the set of \emph{candidates} for the parents the words in $\mathcal W$. For a word $w\in\mathcal W$, $\mathcal Z$ is constructed by splitting $w$ at many points. To capture orthographic changes in the parent word as it undergoes the derivation from $z\in Z$ to $w$ (eg., \emph{believe} $\rightarrow$ \emph{believing}), the type of transformation is kept as part of the candidate. Thus the candidates take the form (\emph{parent}, \emph{type}), where the type is the type of transformation.

Note that parent words usually undergo changes only when attaching suffixes. Thus there is one \emph{Prefix} class, but there are a variety of suffix classes. When acquiring a suffix, the parent word can: (1) undergo no change (\emph{class} $\rightarrow$ \emph{classes}), (2) repeat a character (\emph{star} $\rightarrow$ \emph{starring}), (3) delete a character (\emph{believe} $\rightarrow$ \emph{believing}), (4) modify a character (\emph{parry} $\rightarrow$ \emph{parried}). These correspond to the candidates (\emph{class}, \emph{Suffix}), (\emph{star}, \emph{Repeat}), (\emph{believe}, \emph{Delete}), and (\emph{parry}, \emph{Modify}). Finally, there is a \emph{Stop} type, which is used to signify that the word is a base word.

\subsection{Features}

The model uses a variety of features covering orthographic and semantic aspects of the word-candidate pairs. These features are computed for pairs $(w,z) \in \mathcal W \times \mathcal Z$.

\section{Semi-supervised Learning with Known Word Segmentations}
In addition to maximizing the log-likelihood of the known wordlist,
we can also use explicit feedback regarding the correct segmentation of known words.
From these known segmentations, we can use these segmentations to reconstruct
the morphological chains that would lead to the correct segmentation of the word.

In addition to the neighborhood log-likelihood that we maximize,
we also want to maximize the log-likelihood of the correct segmentation of the training data.
If a given word can be expressed as a morphological chain, the likelihood of its correct segmentation
is equal to the product of the likelihoods of each word-to-parent transition within the morphological chain.

Therefore, the likelihood of the segmentation of the training data is the product of the likelihoods
of each correct word-to-parent transition for every morphological chain in the training data.

Therefore, the likelihood of the training data is given as such:
\begin{equation}
    \prod_{w, c \in D} P(c | w) = \prod_{w, c \in D}\left(\prod_{w', z' \in MC(w)} P(z | w)\right)
\end{equation}

explanation blah
%TODO better explanation

Therefore, the log-likelihood of our training data can be expressed as
\begin{equation}
    \begin{split}
        L_S(\theta; D) &= \log{\prod_{w, z \in D} P(z|w)} \\
                     &= \sum_{w, z \in D} \left(\theta \cdot \phi(w, z) - \log\left(\sum_{z' \in C(w)} e^{\theta \cdot \phi(w, z')} \right)\right)
\end{split}
\end{equation}
where $w, z$ represent every morphological chain transition implied in the training data.

We can also express its gradient as such:
\begin{equation}
    \begin{split}
        \nabla_\theta L_S &= \sum_{w, z \in D} \left(\phi(w, z) - \frac{\sum_{z' \in C(w)} \phi(w, z') e^{\theta \cdot \phi(w, z')} }{\sum_{z' \in C(w)} e^{\theta \cdot \phi(w, z')}}\right)
    \end{split}
\end{equation}

Adding this to our original log-likelihood from contrastive estimation
along with an L2 regularization term, we get our objective:
\begin{equation}
    \begin{split}
        L(\theta; D) &= L_{CE}(\theta; D) + \alpha L_S(\theta; D) - \lambda \| \theta \|^2 \\
        \nabla_\theta L &= \nabla_\theta L_{CE} + \alpha \nabla_\theta L_S - 2\lambda \theta
    \end{split}
\end{equation}
where $\alpha$ is a free parameter that specifies by how much to weight the labeled training data likelihood
against the contrastive estimation likelihood.
As before, with its gradient given above, we minimize $L(\theta, D)$ with LBFGS-B.

\section{Using Cross-Language Features and Candidates}
Due to the simple nature of our parent candidate generation routine,
there are many cases in which the potential parent is not actually generated;
for example, the morphological parent of "feet" is the singular "foot";
however, there is no prefix or suffix that turns one into the other, and
therefore the parent for "feet" is not generated by our system.

However, other languages may not have such an irregular transformation
for the same words, for example in Turkish the corresponding translations
for "feet" and "foot" are "ayaklar" and "ayak". A properly trained Turkish
morphological parent model will correctly detect "ayak" as a parent of "ayaklar",
and can be used to suggest morphological parents in English after translation.

Since there are multiple possible morphological parents for a given word, and multiple
possible translations in either direction, this leads to many possible candidates for a given word,
we generate a set of possible parent candidates (from translation):
\begin{equation}
    C_2(w) = \bigcup_{w_t \in T_{ET}(w)}\left(\bigcup_{w_{tp} \in MP_T (w_t)} T_{TE}(w_{tp})\right)
\end{equation}
where $T_{ET}$ returns the set of possible Turkish translations
for an english word; $MP_T$ returns the set of possible parents for some Turkish word,
$T_{TE}$ returns the set of possible english translations for an english word,
and this potentially large set of candidates is pruned with heuristics judging
how similar the candidate is to the original word. We then extend our original parent candidate
with this parent candidate set.

Additionally, we extend our feature set to include whether or not each candidate was
generated by a turkish translation (a strong indicator for parent-ness) and how confident
the Turkish model was in selecting a parent.

\end{document}
