\documentclass[10pt,twocolumn]{article} 
\usepackage{simpleConference}
\usepackage{amsmath}
\usepackage{times}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{url,hyperref}

\begin{document}

\title{Extending Morphological Chains with Supervised Learning and Cross-Language Features}

\author{Calvin Huang (calvinh@mit.edu)\\
    Brian Shimanuki (bshimanuki@mit.edu)\\
    Charlotte Chen (czchen@mit.edu)
\\
}

\maketitle
\thispagestyle{empty}

\begin{abstract}
    Building on an existing algorithm to perform morphological analysis,
    based on using semantic features and contrastive estimation to detect morphological chains,
    we investigate the effects of various types of extensions to this algorithm.
    We began by adding certain features to the model and heuristics for prefix/suffix selection.
    In addition, we extend the unsupervised model with the log-likelihood of known word segmentations,
    producing a semi-supervised model.
    Finally, we also attempt to use Turkish morphological parents and an English-Turkish translation
    dictionary to help detect irregular English word segmentations and transformations.
\end{abstract}

% TODO explain existing code
\section{Introduction}
Morphologically related words\dots %%TODO

\section{Semi-supervised Learning with Known Word Segmentations}
In addition to maximizing the log-likelihood of the known wordlist,
we can also use explicit feedback regarding the correct segmentation of known words.
From these known segmentations, we can use these segmentations to reconstruct
the morphological chains that would lead to the correct segmentation of the word.

In addition to the neighborhood log-likelihood that we maximize,
we also want to maximize the log-likelihood of the correct segmentation of the training data.
If a given word can be expressed as a morphological chain, the likelihood of its correct segmentation
is equal to the product of the likelihoods of each word-to-parent transition within the morphological chain.

Therefore, the likelihood of the segmentation of the training data is the product of the likelihoods
of each correct word-to-parent transition for every morphological chain in the training data.

Therefore, the likelihood of the training data is given as such:
\begin{equation}
    \prod_{w, c \in D} P(c | w) = \prod_{w, c \in D}\left(\prod_{w', z' \in MC(w)} P(z | w)\right)
\end{equation}

explanation blah
%TODO better explanation

Therefore, the log-likelihood of our training data can be expressed as
\begin{equation}
    \begin{split}
        L_S(\theta; D) &= \log{\prod_{w, z \in D} P(z|w)} \\
                     &= \sum_{w, z \in D} \left(\theta \cdot \phi(w, z) - \log\left(\sum_{z' \in C(w)} e^{\theta \cdot \phi(w, z')} \right)\right)
\end{split}
\end{equation}
where $w, z$ represent every morphological chain transition implied in the training data.

We can also express its gradient as such:
\begin{equation}
    \begin{split}
        \nabla_\theta L_S &= \sum_{w, z \in D} \left(\phi(w, z) - \frac{\sum_{z' \in C(w)} \phi(w, z') e^{\theta \cdot \phi(w, z')} }{\sum_{z' \in C(w)} e^{\theta \cdot \phi(w, z')}}\right)
    \end{split}
\end{equation}

Adding this to our original log-likelihood from contrastive estimation
along with an L2 regularization term, we get our objective:
\begin{equation}
    \begin{split}
        L(\theta; D) &= L_{CE}(\theta; D) + \alpha L_S(\theta; D) - \lambda \| \theta \|^2 \\
        \nabla_\theta L &= \nabla_\theta L_{CE} + \alpha \nabla_\theta L_S - 2\lambda \theta
    \end{split}
\end{equation}
where $\alpha$ is a free parameter that specifies by how much to weight the labeled training data likelihood
against the contrastive estimation likelihood.
As before, with its gradient given above, we minimize $L(\theta, D)$ with LBFGS-B.

\section{Using Cross-Language Features and Candidates}
Due to the simple nature of our parent candidate generation routine,
there are many cases in which the potential parent is not actually generated;
for example, the morphological parent of "feet" is the singular "foot";
however, there is no prefix or suffix that turns one into the other, and
therefore the parent for "feet" is not generated by our system.

However, other languages may not have such an irregular transformation
for the same words, for example in Turkish the corresponding translations
for "feet" and "foot" are "ayaklar" and "ayak". A properly trained Turkish
morphological parent model will correctly detect "ayak" as a parent of "ayaklar",
and can be used to suggest morphological parents in English after translation.

Since there are multiple possible morphological parents for a given word, and multiple
possible translations in either direction, this leads to many possible candidates for a given word,
we generate a set of possible parent candidates (from translation):
\begin{equation}
    C_2(w) = \bigcup_{w_t \in T_{ET}(w)}\left(\bigcup_{w_{tp} \in MP_T (w_t)} T_{TE}(w_{tp})\right)
\end{equation}
where $T_{ET}$ returns the set of possible Turkish translations
for an english word; $MP_T$ returns the set of possible parents for some Turkish word,
$T_{TE}$ returns the set of possible english translations for an english word,
and this potentially large set of candidates is pruned with heuristics judging
how similar the candidate is to the original word. We then extend our original parent candidate
with this parent candidate set.

Additionally, we extend our feature set to include whether or not each candidate was
generated by a turkish translation (a strong indicator for parent-ness) and how confident
the Turkish model was in selecting a parent.

\end{document}
