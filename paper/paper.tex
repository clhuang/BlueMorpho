\documentclass[10pt,twocolumn]{article} 
\usepackage{simpleConference}
\usepackage{amsmath}
\usepackage{times}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{url,hyperref}

\begin{document}

\title{Extending Morphological Chains with Supervised Learning and Cross-Language Features}

\author{Calvin Huang (calvinh@mit.edu)\\
    Brian Shimanuki (bshimanuki@mit.edu)\\
    Charlotte Chen (czchen@mit.edu)
\\
}

\maketitle
\thispagestyle{empty}

\begin{abstract}
    Building on an existing algorithm to perform morphological analysis,
    based on using semantic features and contrastive estimation to detect morphological chains,
    we investigate the effects of various types of extensions to this algorithm.
    We began by adding certain features to the model and heuristics for prefix/suffix selection.
    In addition, we extend the unsupervised model with the log-likelihood of known word segmentations,
    producing a semi-supervised model.
    Finally, we also attempt to use Turkish morphological parents and an English-Turkish translation
    dictionary to help detect irregular English word segmentations and transformations.
\end{abstract}

% TODO explain existing code

\section{Semi-supervised Learning with Known Word Segmentations}
From known word segmentations, we can use these segmentations to reconstruct
the morphological chains that would lead to the correct segmentation of the word.

In addition to the neighborhood log-likelihood that we maximize,
we also want to maximize the log-likelihood of the correct segmentation of the training data.
If a given word can be expressed as a morphological chain, the likelihood of its correct segmentation
is equal to the product of the likelihoods of each word-to-parent transition within the morphological chain.

Therefore, the likelihood of the segmentation of the training data is the product of the likelihoods
of each correct word-to-parent transition for every morphological chain in the training data.

Therefore, the likelihood of the training data is given as such:
\begin{equation}
    \prod_{w, c \in D} P(c | w) = \prod_{w, c \in D}\left(\prod_{w', z' \in MC(w)} P(z | w)\right)
\end{equation}

explanation blah
%TODO better explanation

Therefore, the log-likelihood of our training data can be expressed as
\begin{equation}
    \begin{split}
        L_S(\theta; D) &= \log{\prod_{w, z \in D} P(z|w)} \\
                     &= \sum_{w, z \in D} \left(\theta \cdot \phi(w, z) - \log\left(\sum_{z' \in C(w)} e^{\theta \cdot \phi(w, z')} \right)\right)
\end{split}
\end{equation}
where $w, z$ represent every morphological chain transition implied in the training data.

We can also express its gradient as such:
\begin{equation}
    \begin{split}
        \nabla_\theta L_S &= \sum_{w, z \in D} \left(\phi(w, z) - \frac{\sum_{z' \in C(w)} \phi(w, z') e^{\theta \cdot \phi(w, z')} }{\sum_{z' \in C(w)} e^{\theta \cdot \phi(w, z')}}\right)
    \end{split}
\end{equation}

Adding this to our original log-likelihood from contrastive estimation
along with an L2 regularization term, we get our objective:
\begin{equation}
    \begin{split}
        L(\theta; D) &= L_{CE}(\theta; D) + \alpha L_S(\theta; D) - \lambda \| \theta \|^2 \\
        \nabla_\theta L &= \nabla_\theta L_{CE} + \alpha \nabla_\theta L_S - 2\lambda \theta
    \end{split}
\end{equation}
where $\alpha$ is a free parameter that specifies by how much to weight the labeled training data likelihood
against the contrastive estimation likelihood.
As before, with its gradient given above, we minimize $L(\theta, D)$ with LBFGS-B.


\section{Using \LaTeX with PDF Figures}

This is a sample document for use with pdflatex, which is
a program that is included with the Miktex distribution
that directly produces PDF files from \LaTeX sources.
To run \LaTeX on this file, you need the following files:
\begin{enumerate}
\item templatePDF.tex (this file)
\item figure.pdf (the figure file)
\item simpleConference.sty (style file)
\item refs.bib (bibiliography file)
\end{enumerate}
\noindent
To create a PDF file, execute the following commands:
\begin{enumerate}
\item pdflatex templatePDF
\item bibtex templatePDF
\item pdflatex templatePDF
\item pdflatex templatePDF
\end{enumerate}
\noindent
Yes (strangely) it is necessary to run pdflatex three times.
The result will be a PDF file (plus several other files that \LaTeX
produces).  You will need a mechanism, of course, for executing
commands on the command line. If you are using Windows, I recommend
installing Cygwin and using its bash shell.

\section{How to Include Vergil Diagrams as Figures}

Suppose you wish to include a figure, like that in figure \ref{fig-label}.
The simplest mechanism is to install Adobe Acrobat, which includes
a ``printer'' called ``Acrobat Distiller.'' Printing to this printer
creates a PDF file, which can be included in a document as shown
here.  To include Ptolemy II models \cite{PtolemyVol1:04},
just print to the distiller from within Vergil and reference
the PDF file in your \LaTeX document.

There is a bit more work to do, however.
The file that is produced by the distiller represents
a complete page, not the individual figure.
You can open it in using Acrobat (version 5.0 or later),
and select Document $\rightarrow$ Crop Pages from the menu.
In the resulting dialog, check ``Remove White Margins.''
Save the modified PDF file in a file and then reference
it in the \LaTeX file as shown in this example.

An alternative is to generate EPS (encapsulated postscript),
but the process is much more complex and fragile.
I recommend using pdflatex and Adobe Acrobat.

\bibliographystyle{abbrv}
\bibliography{refs}
\end{document}
