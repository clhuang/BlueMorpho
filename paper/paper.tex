\documentclass[11pt,twocolumn]{article} 
\usepackage{simpleConference}
\usepackage{amsmath}
\usepackage{times}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{url,hyperref}

\begin{document} 
\title{Extending Morphological Chains with Supervised Learning and Cross-Language Features}

\author{Calvin Huang (calvinh@mit.edu)\\
    Brian Shimanuki (bshimanuki@mit.edu)\\
    Charlotte Chen (czchen@mit.edu)
\\
}

\maketitle
\thispagestyle{empty}

\begin{abstract}
    Building on an existing algorithm to perform morphological analysis,
    based on using semantic features and contrastive estimation to detect morphological chains,
    we investigate the effects of various types of extensions to this algorithm.
    We began by adding certain features to the model and heuristics for prefix/suffix selection.
    In addition, we extend the unsupervised model with the log-likelihood of known word segmentations,
    producing a semi-supervised model.
    Finally, we also attempt to use Turkish morphological parents and an English-Turkish translation
    dictionary to help detect irregular English word segmentations and transformations.
\end{abstract}

% TODO explain existing code
\section{Introduction}
The division of a word into morphemes--basic units of meaning--- encodes information about
something
When performing morphological analysis, there are a few different cases to consider: words that share similar
meaning but appear in diffrent syntactic contexts (``German" ---``Germanic"), new words generated from
the combination of two separate words (``bookcase"|``book"---``case"), words with dissimilar meaning but
derived from one another ``insuffcient"---``sufficient"). In the first and last case, words are often related
by the addition of preffixes, suffixes, and other affixes. This allows us to form morphological chains - which
link a base word to its derivative through a series of affixations.
We wish to 

%%TODO

\section{Unsupervised Model}
Our model builds on work done by Narasimhan et al. (2015) in constructing \emph{morphological chains} to model the morphological segmentation of words in a language in an unsupervised manner. The idea behind morphological chains is that complex words are constructed by attaching morphemes simpler words. Thus, complex words have words from which they are directly derived, which we call \emph{parent} words. Likewise, the derived word is a \emph{child} word. A morphological chain is a sequence of words such that consecutive pairs form a parent-child relationship. For example, the word \emph{unsustainable} can be constructed from \emph{sustain}, as demonstrated in the chain \emph{sustain} $\rightarrow$ \emph{sustainable} $\rightarrow$ \emph{unsustainable}. Words without parents are called \emph{base} words. In our example, \emph{sustain} is a base word.

Since a parent word can have multiple children (e.g., \emph{play} $\rightarrow$ \emph{plays} and \emph{play} $\rightarrow$ \emph{played}), words can be part of multiple chains. Thus multiple chains can share segments. Narasimhan makes use of this shared information by constructing a model which analyzes parent-child pairs.

Because we have chains where different chains can share segments, it is natural to consider the graph formulation where each word is represented by a node and each parent-child transition is represented by a directed edge. Then if we consider only the most likely parent for a word,\footnote{Words are not restricted to having a single valid parent (or segmentation). In cases where there are multiple parents (segmentations), we care only that the algorithm selects one of them.} the resulting graph forms a forest, and our objective is to find the path to from a given node to the root in an unsupervised manner without any explicit information about any of the edges.

Note that by constructing morphological chains rather than just finding segmentations, more information is gained about the words tested. Generating the chain, in addition to segmenting a word, also finds the base word and predicts the order in which the morphemes attach to the base word.

\subsection{Model}

In the unsupervised version of our model, we observe words in a wordlist with their word count of occurrences. Additionally, we have access to a large corpus of text from which we can obtain semantic information. Our objective is to generate morphological segmentations, which we do by generating morphological chains, as given by Narasimhan et al.

A log-linear model is used to evaluate different pairs. For this, a feature mapping $\phi: \mathcal W \times \mathcal Z \rightarrow \mathbb R^d$ to compute a corresponding weight vector $\theta\in\mathbb R^d$. $\mathcal W$ is the set of words being trained on, and $\mathcal Z$ is the set of \emph{candidates} for the parents the words in $\mathcal W$. For a word $w\in\mathcal W$, $\mathcal Z$ is constructed by splitting $w$ at many points. To capture orthographic changes in the parent word as it undergoes the derivation from $z\in Z$ to $w$ (eg., \emph{believe} $\rightarrow$ \emph{believing}), the type of transition is kept as part of the candidate. Thus the candidates take the form (\emph{parent}, \emph{type}), where the type is the type of transition.

Note that parent words usually undergo changes only when attaching suffixes. Thus there is one \emph{Prefix} class, but there are a variety of suffix classes. When acquiring a suffix, the parent word can:
\begin{enumerate}
    \item undergo no change (\emph{bike} $\rightarrow$ \emph{bikes})
    \item repeat a character (\emph{star} $\rightarrow$ \emph{starring})
    \item delete a character (\emph{believe} $\rightarrow$ \emph{believing})
    \item modify a character (\emph{parry} $\rightarrow$ \emph{parried}).
\end{enumerate}
These correspond to the candidates (\emph{bike}, \emph{Suffix}), (\emph{star}, \emph{Repeat}), (\emph{believe}, \emph{Delete}), and (\emph{parry}, \emph{Modify}). Finally, there is a \emph{Stop} type, which is used to signify that the word is a base word.

\subsection{Word Vectors}

% TODO wordvector citation?
It has been shown that words can be mapped to word vectors in a reasonably-sized dimensional space such that semantically similar words align in similar directions. Thus we can compute a \emph{cosine similarity} as a distance metric between words which captures semantic similarity. Given vector representations of two words, the cosine similarity is measured as the dot product between the word vectors of those words.

In our work, we generate 200-dimensional vectors for words by using Word2Vec, which looks at word alignment  and co-occurrence patterns across many documents. In generating our vectors, we use text from Wikipedia,
to generate the english word vectors, along with the BOUN corpus of news articles to generate Turkish word vectors.

\subsection{Features}

The model uses a variety of features covering orthographic and semantic aspects of the word-candidate pairs. These features are computed for pairs $(w,z) \in \mathcal W \times \mathcal Z$.

\begin{description}
    \item[Affixes] Prefixes and suffixes appear in many words. For a given word with its potential parent, the affix removed can be compared against a precompiled list of potential affixes. This allows the model to learn which affixes are real and apply them to other cases with the same potential affix.
    \item[Affix Correlation] Similar to comparing affixes across words, we can use the joint distribution of words and their potential affixes. There is a correlation between affixes that can usually attach to the same stem, often corresponding to the part of speech. For example, the participles formed from the suffixes -ing and -ed usually occur together. For candidates with an affix that is correlated with another affix, we can check if the parent with the other affix also occurs in the observed wordlist.
    \item[Semantic Similarity] Morphologically related words should have similar meanings. We measure the cosine similarity between the word and its candidate parent as a feature to represent semantic similarity.
    \item[Transformations] As discussed above, some derivations involve a change to the parent word. To allow for non-concatenative morphology, we use binary features which capture the type of transformation. We use the same types of transformations for features as we do for generating candidates: repetitions, deletions, and modifications. The set of binary features are the cartesian product of the type of transformation with the characters transformed.
    \item[Wordlist] Most of the time, we want the parents to be valid words. To this effect, we compare the candidate parent against the wordlist, and add a feature for the log of the word count. Additionally, we set a binary feature corresponding to whether the word was found at all in the wordlist.
    \item[Stop Features] We have a number of miscellaneous features targeted at finding the end of chains. These include orthographic information like the length of the parent, unigrams and bigrams at the beginning and end of the parent, and the highest cosine similarity between the word and any of its candidate parents.
\end{description}

\subsection{Unsupervised Learning}
Recall that in our model, we want to find weights $\theta$ for the feature vectors $\phi(w,z)$, where $w\in\mathcal W$ and $z\in\mathcal Z$. Define the probabilities of a word-candidate pair $(w,z)$ as $P(w,z)\propto e^{\theta\cdot\phi(w,z)}$. Then the probability of a candidate $z$ ocuring given $w$ is
\begin{equation}
    \label{equ:prob_par}
    P(z|w) = \frac{e^{\theta\cdot\phi(w,z)}} {\sum_{z'\in C(w)} e^{\theta\cdot\phi(w,z')}}
\end{equation}

Let our set of observed words be $D$. We approach this by trying to maximize the likelihood of observing the words in $D$ from the space of all constructible strings from the alphabet, $\Sigma^*$. We use the EM-algorithm to iteratively maximize the likelihood over $\theta$ as given by

\begin{equation}
    \begin{split}
        L(\theta; D) &= \prod_{w^*\in D} P(w^*) \\
        &= \prod_{w^*\in D} \sum_{z\in C(w^*)} P(w^*, z) \\
        &= \prod_{w^*\in D} \frac{\sum_{z\in C(w^*)} e^{\theta\cdot\phi(w^*,z)}} {\sum_{w\in\Sigma^*}\sum_{z\in C(w) e^{\theta\cdot\phi(w,z)}}} \\
    \end{split}
\end{equation}

We cannot compute $L(\theta; D)$ directly since we cannot compute over all strings in $\Sigma^*$. Instead, we approximate this distribution by considering only strings which are similar to those encountered in $D$.

We use the method of Contrastive Estimation (Smith and Eisner, 2005) and substitute the space of all strings $\Sigma^*$ with neighbors of each word, $N(w)$. Toward this end, we transpose pairs of consecutive letters of $w$ near both ends of the word. We also do both simultaneously. Together these form our set of neighbors for $w$. The neighbors form a proxy for $\Sigma^*$, and represent the set of strings we want to reduce the probability of seeing in our model because they are not observed. This has the benefit of providing contrast in the structure of words while not requiring the model to look at the entire $\Sigma^*$.

With this substitution, we can formulate the contrastive likelihood as
\begin{equation}
    L_C(\theta; D) = \prod_{w^*\in D} \frac{\sum_{z\in C(w^*)} e^{\theta\cdot\phi(w^*,z)}} {\sum_{w\in N(w^*)} \sum_{z\in C(w)} e^{\theta\cdot\phi(w,z)}}
\end{equation}

With a regularization term, this becomes
\begin{equation}
    \begin{split}
        L_C(\theta; D) &= \prod_{w^*\in D} \frac{\sum_{z\in C(w^*)} e^{\theta\cdot\phi(w^*,z)}} {\sum_{w\in N(w^*)} \sum_{z\in C(w)} e^{\theta\cdot\phi(w,z)}} \\
        &\qquad - \lambda \|\theta\|^2
    \end{split}
\end{equation}

Our EM-algorithm will then operate on the log-likelihood, which takes the following form:
\begin{equation}
    \begin{split}
        L_L(\theta; D) &= \sum_{w^*\in D} \Bigg(\log \sum_{z\in C(w^*)} e^{\theta\cdot\phi(w^*,z)} \Bigg.\\
        &\qquad\Bigg. -\log \sum_{w\in N(w^*)} \sum_{z\in C(w)} e^{\theta\cdot\phi(w,z)} \Bigg) \\
        &\qquad - \lambda \|\theta\|^2
    \end{split}
\end{equation}

This has the gradient given by
\begin{equation}
    \begin{split}
        &\frac{\partial L_L(\theta; D)}{\partial \theta_j} \\
        &= \sum_{w^*\in D} \Bigg( \frac{\sum_{z\in C(w^*)} \phi_j(w^*,z)\cdot e^{\theta\cdot\phi(w^*,z)}} {\sum_{z\in C(w^*)} e^{\theta\cdot\phi(w^*,z)}} \Bigg. \\
        &\qquad -\frac{\sum_{w\in N(w^*)} \sum_{z\in C(w)} \phi_j(w,z)\cdot e^{\theta\cdot\phi(w,z)}} {\sum_{w\in N(w^*)} \sum_{z\in C(w)} e^{\theta\cdot\phi(w,z)}} \Bigg) \\
        &\qquad -2\lambda\theta_j
    \end{split}
\end{equation}

We maximize $L_L(\theta; D)$ with its gradient using the LBFGS-B algorithm (Byrd et al., 1995).

\subsection{Prediction}
The EM-algorithm yields an optimal $\theta^*$ of weights of the features generated from $D$. Then given a test word $w$, we can predict the probability of a given parent candidate $z$ by computing $P(z|w)=e^{\theta^*\cdot\phi(w,z)}$ as in \autoref{equ:prob_par}. To predict the parent of $w$, we pick the MLE candidate from parent candidates generated for $w$, which we can generate in the same way as we did while training.

To generate a morphological chain for $w$, we recursively predict the MLE parent for $w$, then the MLE parent of that prediction, and so forth until the \emph{Stop} candidate is predicted. Together, this sequence forms our chain. A morphological segmentation can be constructed from the chain by splitting $w$ at every point corresponding to where each of the edges in the chain splits its child word.

\section{Semi-supervised Learning with Known Word Segmentations}
In a semi-supervised learning model, we still have access to a large wordlist of observed words as well as the corpus for semantic information. In addition, we have a small amount of pre-segmented words (about 2\% of the size of the wordlist). We use these pre-segmented words to generate

In addition to maximizing the log-likelihood of the known wordlist,
we can also use explicit feedback regarding the correct segmentation of known words.
From these known segmentations, we can use these segmentations to reconstruct
the morphological chains that would lead to the correct segmentation of the word.

In addition to the neighborhood log-likelihood that we maximize,
we also want to maximize the log-likelihood of the correct segmentation of the training data.
If a given word can be expressed as a morphological chain, the likelihood of its correct segmentation
is equal to the product of the likelihoods of each word-to-parent transition within the morphological chain.

Therefore, the likelihood of the segmentation of the training data is the product of the likelihoods
of each correct word-to-parent transition for every morphological chain in the training data.

Therefore, the likelihood of the training data is given as such:
\begin{equation}
    \prod_{w, c \in D} P(c | w) = \prod_{w, c \in D}\left(\prod_{w', z' \in MC(w)} P(z | w)\right)
\end{equation}

We want to maximize the probability of the document $D$, where $w$ is a word in $D$ and $c$ is
its segmentation. As above, we 
%TODO better explanation

Therefore, the log-likelihood of our training data can be expressed as
\begin{equation}
    \begin{split}
        L_S(\theta; D) &= \log{\prod_{w, z \in D} P(z|w)} \\
                       &= \sum_{w, z \in D} \Bigg(\theta \cdot \phi(w, z) \Bigg. \\
                       &\qquad\Bigg. - \log\left(\sum_{z' \in C(w)} e^{\theta \cdot \phi(w, z')} \right)\Bigg)
   \end{split}
\end{equation}
where $w, z$ represent every morphological chain transition implied in the training data.

We can also express its gradient as such:
\begin{equation}
    \begin{split}
        \nabla_\theta L_S &= \sum_{w, z \in D} \Bigg(\phi(w, z) \Bigg. \\
                          &\qquad\left. - \frac{\sum_{z' \in C(w)} \phi(w, z') e^{\theta \cdot \phi(w, z')} }{\sum_{z' \in C(w)} e^{\theta \cdot \phi(w, z')}}\right)
    \end{split}
\end{equation}

Adding this to our original log-likelihood from contrastive estimation
along with an L2 regularization term, we get our objective:
\begin{equation}
    \begin{split}
        L(\theta; D) &= L_{CE}(\theta; D) + \alpha L_S(\theta; D) - \lambda \| \theta \|^2 \\
        \nabla_\theta L &= \nabla_\theta L_{CE} + \alpha \nabla_\theta L_S - 2\lambda \theta
    \end{split}
\end{equation}
where $\alpha$ is a free parameter that specifies by how much to weight the labeled training data likelihood
against the contrastive estimation likelihood.
As before, with its gradient given above, we minimize $L(\theta, D)$ with LBFGS-B.

\section{Using Cross-Language Features and Candidates}
Due to the simple nature of our parent candidate generation routine,
there are many cases in which the potential parent is not actually generated;
for example, the morphological parent of ``feet'' is the singular ``foot'';
however, there is no prefix or suffix that turns one into the other, and
therefore the parent for ``feet'' is not generated by our system.

However, other languages may not have such an irregular transformation
for the same words, for example in Turkish the corresponding translations
for ``feet'' and ``foot'' are ``ayaklar'' and ``ayak''. A properly trained Turkish
morphological parent model will correctly detect ``ayak'' as a parent of ``ayaklar'',
and can be used to suggest morphological parents in English after translation.

Since there are multiple possible morphological parents for a given word, and multiple
possible translations in either direction, this leads to many possible candidates for a given word,
we generate a set of possible parent candidates (from translation):
\begin{equation}
    C_2(w) = \bigcup_{w_t \in T_{ET}(w)}\left(\bigcup_{w_{tp} \in MP_T (w_t)} T_{TE}(w_{tp})\right)
\end{equation}
where $T_{ET}$ returns the set of possible Turkish translations
for an english word; $MP_T$ returns the set of possible parents for some Turkish word,
$T_{TE}$ returns the set of possible english translations for an english word,
and this potentially large set of candidates is pruned with heuristics judging
how similar the candidate is to the original word. We then extend our original parent candidate
with this parent candidate set.

Additionally, we extend our feature set to include whether or not each candidate was
generated by a turkish translation (a strong indicator for parent-ness) and how confident
the Turkish model was in selecting a parent.

\section{Experimental Validation}

\subsection{Data}

We used the datasets provided by 2010 Morpho Challenges. For English, this includes
a wordlist with word frequencies of approximately 878,000 words scraped from the
Wortschatz collection from the University of Leipzig, CLEF, and the Europarl corpus
as well as gold standard segmentation training and development sets of approximately 
1000 randomly selected words. Gold standard segmentation is based on the CELEX database.
For Turkish, the word list contains approximately 670,000 words, with gold standards
from a morphological parser developed at Boğaziçi University. The Turkish wordlist
was obtained from a significantly smaller corpus --- at 1 million words, compared
to the English corpus of 18 million.

We tested on 4 different sizes of wordvectors:
of wordvector 
small is 2.5k
med is 
large is 160k
full is 878k

\subsection{Evaluation Model}

\begin{table*}
    \begin{center}
        \begin{tabular}{ | l | l | l | l | l |}
            \hline
            Language & Method & Precision & Recall & F1 \\ \hline
                     & Narasimhan & 0.807 & 0.722 & 0.762 \\ 
                     & Unsupervised & -  & - & - \\ 
            English & Semi-supervised & - & - & - \\ 
                    & Two language unsupervised & - & - & - \\ 
                    & Two language semi-supervised & - & - & - \\ \hline
                    & Narasimhan & 0.743 & 0.520 & 0.612 \\ 
                    & Unsupervised & - & - & - \\ 
            Turkish & Semi-supervised & - & - & -  \\ 
                    & Two language unsupervised  & - & - & -  \\ 
                    & Two language semi-supervised  & - & - & -  \\ \hline
        \end{tabular}
        \caption{Results 'n shit TODO}
    \end{center}
\end{table*}

\begin{table*}
    \begin{center}
        \begin{tabular}{ | l | l | l | l | l | l |}
            \hline
            Language & \multicolumn{2}{|c|}{\textbf{Correct Segmentations}} & \multicolumn{3}{|c|}{\textbf{Incorrect Segmentations}}\\ \hline
                     & Word & Segmentation & Word & Predicted & Correct\\ \hline
            English & suburbanite & suburb/an/ite & provokingly & provok/ing/ly & provoking/ly  \\ 
                    & buffeted & buffeted & -  & - & -\\ 
                    & ewers' &  ewer/s/' & - & - & -\\ 
                    & yowling & yowl/ing & - & -& - \\ 
                    & Two language semi-supervised & - & - & - & -\\ \hline
                    & Narasimhan & 0.743 & 0.520 & 0.612 & -\\ 
                    & Unsupervised & - & - & - & -\\ 
            Turkish & Semi-supervised & - & - & -  & -\\ 
                    & Two language unsupervised  & - & - & -  & -\\ 
                    & Two language semi-supervised  & - & - & -  & -\\ \hline
        \end{tabular}
        \caption{Results 'n shit TODO}
    \end{center}
\end{table*}

\end{document}
